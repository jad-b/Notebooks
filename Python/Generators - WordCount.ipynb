{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# \n",
    "# We'll recreate a real-time streaming system, a la Apache Storm,\n",
    "# using Python's generators. Why? Because it's more interesting this\n",
    "# way.\n",
    "#\n",
    "# Quick terminology:\n",
    "#  Stream: Data input.\n",
    "#  Spout: Stream creator.\n",
    "#  Bolt: Stream processor. Outputs a modified stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import string\n",
    "\n",
    "# Logging was being weird inside my iPython notebook, so I resorted to print()\n",
    "# LOGGER = logging.getLogger('WordCount')\n",
    "# LOGGER.setLevel(logging.INFO)\n",
    "# LOGGER.addHandler(logging.StreamHandler())\n",
    "# LOGGER.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizers convert file streams into streams of delimited values.\n",
    "    \n",
    "    :ivar str filename: Name of file to read from.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, delimiter='.'):\n",
    "        self.filename = filename\n",
    "        self.setup()\n",
    "        try:\n",
    "            english_tokens = os.path.join(nltk_data_parent_dir,\n",
    "                                         'tokenizers/punkt/english.pickle')\n",
    "            self.tokenizer = nltk.data.load(english_tokens)\n",
    "        except:\n",
    "            logger.exception('Failed to load tokenizer data from %s', \n",
    "                             english_tokens)\n",
    "            raise\n",
    "            \n",
    "    @staticmethod\n",
    "    def setup():    \n",
    "        nltk_data_parent_dir = os.path.expanduser('~')\n",
    "        if 'nltk_data' not in os.listdir(nltk_data_parent_dir):\n",
    "            import nltk\n",
    "            nltk.download('punkt')\n",
    "\n",
    "        \n",
    "    def tokenize(self):\n",
    "        # This is dangerous if the file is very large. If memory was a bottlneck,\n",
    "        # we could read in bytes until we hit the delimiter, then pass that to \n",
    "        # the Mapper's input queue.\n",
    "        with open(self.filename, 'r') as data:\n",
    "            return self.tokenizer.tokenize(data)\n",
    "        \n",
    "    def tokenize_text(text):\n",
    "        return self.tokenizer.tokenize(data)\n",
    "    \n",
    "    def stream_tokens(self):\n",
    "        for token in self.tokenize():\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_paragraph = \"\"\"As Mr. Smith walked towards the edge of the cliff, \n",
    "he recalled what his father had said to him. \"Boy,\", his father had started, \n",
    "\"I must tell you this next thing before I go.\" But Mr. Smith had never \n",
    "gotten to hear what his father had to say, as at that moment his father\n",
    "had fallen over the side of the cliff. The very same cliff, Mr. Smith mused,\n",
    "that he himself was walking towards this very moment. How Ms. Smith would\n",
    "laugh, he thought to himself, if she were reading an account of his present\n",
    "actions.\n",
    "\"\"\"\n",
    "test_sentences = [\n",
    "    'As Mr. Smith walked towards the edge of the cliff, he recalled what his father had said to him.', \n",
    "    '\"Boy,\", his father had started, \"I must tell you this next thing before I go.\"',\n",
    "    'But Mr. Smith had never gotten to hear what his father had to say, as at that moment his father had fallen over the side of the cliff.',\n",
    "    'The very same cliff, Mr. Smith mused, that he himself was walking towards this very moment.', \n",
    "    '\"How Ms. Smith would laugh,\" he thought to himself, \"if she were reading an account of his present actions.\"'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_sentences(stream):\n",
    "    \"\"\"Split sentences into word tokens, bereft of punctuation and lowercase.\"\"\"\n",
    "    for s, idx in stream:\n",
    "        # LOGGER.info(\"split_bolt: %s\", s)\n",
    "        words = (w.strip(string.punctuation).lower() for w in s.split())\n",
    "        for word in words:\n",
    "            # LOGGER.debug(\"split_bolt: Yielding %s, %d\", word, idx)\n",
    "            yield (word, idx)\n",
    "            \n",
    "def tally_words(stream, table):\n",
    "    \"\"\"Associate words with sentence indices and tally their appearances.\"\"\"\n",
    "    for word, idx in stream:\n",
    "        table[word].append(idx)\n",
    "        # LOGGER.debug(\"tally_bolt: Yielding %s, %d, %s\", word, idx, table[word])\n",
    "        yield word, len(table[word]), table[word]\n",
    "        \n",
    "def count_words(stream):\n",
    "    \"\"\"Archive word counts from previous stream.\n",
    "    \n",
    "    To continue the theme of stream processing, I could've had this itself\n",
    "    be a stream, maintaining an organized data structure of words. The output\n",
    "    would be the most recently updated (word, count, sentence indices) tuple.\n",
    "    We'd need to implement a data structure with good lookup and ordered insertions\n",
    "    times, such as a binary tree. The `bisect` library provides a working example \n",
    "    for doing so.\n",
    "    \"\"\"\n",
    "    table = {}\n",
    "    for word, count, indices in stream:\n",
    "        table[word] = (count, indices)\n",
    "    return table\n",
    "\n",
    "\n",
    "def pretty_streamer(word_hash):\n",
    "    for word, val in word_hash.items():\n",
    "        yield '{}: {} [{}]'.format(word,\n",
    "                                   val[0], \n",
    "                                   ', '.join([str(x) for x in val[1]]))\n",
    "                     \n",
    "def write_to_file(inputs):\n",
    "    with open('concordance.txt', 'w') as fp:\n",
    "        for line in inputs:\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account: 1 [4]\n",
      "actions: 1 [4]\n",
      "an: 1 [4]\n",
      "as: 2 [0, 2]\n",
      "at: 1 [2]\n",
      "before: 1 [1]\n",
      "boy: 1 [1]\n",
      "but: 1 [2]\n",
      "cliff: 3 [0, 2, 3]\n",
      "edge: 1 [0]\n",
      "fallen: 1 [2]\n",
      "father: 4 [0, 1, 2, 2]\n",
      "go: 1 [1]\n",
      "gotten: 1 [2]\n",
      "had: 5 [0, 1, 2, 2, 2]\n",
      "he: 3 [0, 3, 4]\n",
      "hear: 1 [2]\n",
      "him: 1 [0]\n",
      "himself: 2 [3, 4]\n",
      "his: 5 [0, 1, 2, 2, 4]\n",
      "how: 1 [4]\n",
      "i: 2 [1, 1]\n",
      "if: 1 [4]\n",
      "laugh: 1 [4]\n",
      "moment: 2 [2, 3]\n",
      "mr: 3 [0, 2, 3]\n",
      "ms: 1 [4]\n",
      "mused: 1 [3]\n",
      "must: 1 [1]\n",
      "never: 1 [2]\n",
      "next: 1 [1]\n",
      "of: 3 [0, 2, 4]\n",
      "over: 1 [2]\n",
      "present: 1 [4]\n",
      "reading: 1 [4]\n",
      "recalled: 1 [0]\n",
      "said: 1 [0]\n",
      "same: 1 [3]\n",
      "say: 1 [2]\n",
      "she: 1 [4]\n",
      "side: 1 [2]\n",
      "smith: 4 [0, 2, 3, 4]\n",
      "started: 1 [1]\n",
      "tell: 1 [1]\n",
      "that: 2 [2, 3]\n",
      "the: 5 [0, 0, 2, 2, 3]\n",
      "thing: 1 [1]\n",
      "this: 2 [1, 3]\n",
      "thought: 1 [4]\n",
      "to: 4 [0, 2, 2, 4]\n",
      "towards: 2 [0, 3]\n",
      "very: 2 [3, 3]\n",
      "walked: 1 [0]\n",
      "walking: 1 [3]\n",
      "was: 1 [3]\n",
      "were: 1 [4]\n",
      "what: 2 [0, 2]\n",
      "would: 1 [4]\n",
      "you: 1 [1]\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    LOGGER.setLevel(logging.DEBUG)\n",
    "    # Mock text stream\n",
    "    spout = ((s, i) for i, s in enumerate(test_sentences))\n",
    "    # First bolt: Convert sentences into (word, sentence #)\n",
    "    split_bolt = split_sentences(spout)\n",
    "    # Second bolt: Tally words up \n",
    "    tally_hash = defaultdict(list)\n",
    "    tally_bolt = tally_words(split_bolt, tally_hash)\n",
    "    # Terminal: Maintain a structure of words and their counts\n",
    "    word_count = count_words(tally_bolt)\n",
    "    ordered_count = OrderedDict(sorted(word_count.items(), key=lambda x: x[0]))\n",
    "    \n",
    "    from pprint import pprint\n",
    "    print('\\n'.join([x for x in pretty_streamer(ordered_count)]))\n",
    "    write_to_file(pretty_streamer(ordered_count))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
