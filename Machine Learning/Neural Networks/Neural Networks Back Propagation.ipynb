{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.optimize import fmin_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74086829  0.94067339  0.34084823  0.05432868]\n",
      " [ 0.79127816  0.92556804  0.27367472  0.06546035]\n",
      " [ 0.80134333  0.91853201  0.25884502  0.07100224]\n",
      " [ 0.74134848  0.94022222  0.34003037  0.05471739]\n",
      " [ 0.78070552  0.92968906  0.28812528  0.06239245]\n",
      " [ 0.80825702  0.91575985  0.2495236   0.07297472]\n",
      " [ 0.74414057  0.93918548  0.33618091  0.05554377]\n",
      " [ 0.77016492  0.93342417  0.30244605  0.05959466]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-eeea42b9945d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_p\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mtest_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-eeea42b9945d>\u001b[0m in \u001b[0;36mtest_forward\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mmax_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mexp_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_p\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[0mtest_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def forward(x, *Thetas):\n",
    "    \"\"\"Put a training sample through the network.\n",
    "    \n",
    "    Each layer's activation units will be calculated. Said another way,\n",
    "    the data point will be fed into the NN as the input layer. The input\n",
    "    will be \"fed\" into each unit in each layer by multiplying against the\n",
    "    unit's weight vector. The result is the unit having an output \n",
    "    \"activation\" value. \n",
    "    \n",
    "    This activation value roughly corresponds to its predictions for the \n",
    "    given sample. As the size of layers drifts from the original number of\n",
    "    features this becomes more abstract; in fact, you could say each layer\n",
    "    in the NN is a higher-level abstraction of the previous level. The\n",
    "    output layer presents the _final_ abstraction, reducing _n_ features\n",
    "    into a prediction across _K_ classes, in the case of a classification \n",
    "    problem.\n",
    "    \n",
    "    :arg vector x: (m x 1) row vector representing a data point with \n",
    "        _m_ features. It is assumed the bias value has _not_ been inserted.\n",
    "    :arg [matrix] Theta: Matrices of weights for the connections between \n",
    "        layers. Size should be L-1, where L is the number of layers.\n",
    "    :rtype: 2D array\n",
    "    :return: a 2D array of activation values. Each row represents a layer, \n",
    "        and columns represent units in the array. Thus, the output layer's\n",
    "        activations can be found at ``return_arr[-1]``.\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1, \"Expected row vector, got {}\".format(x)\n",
    "    #: Activation units\n",
    "    # Since 2D numpy arrays must have the same dimensions, we have to\n",
    "    # our layer activation units in a normal Python list.\n",
    "    a_units = list([x])\n",
    "#     print(a_units)\n",
    "    for i in range(len(Thetas)):\n",
    "        # Add the bias value\n",
    "        biased_a = np.append(np.ones(1), a_units[i])\n",
    "#         assert biased_a.shape == (3,), \"a_bias = {}\".format(biased_a.shape)\n",
    "#         print(biased_a)\n",
    "        # Compute (a_i * Theta.T)\n",
    "#         print(Thetas[i].T)\n",
    "        z = biased_a.dot(Thetas[i].T)\n",
    "#         print(z)\n",
    "        a_units.append(sigmoid(z.T))\n",
    "#         print(a_units)\n",
    "    return a_units\n",
    "    \n",
    "    \n",
    "def test_forward():\n",
    "    # 2 features => 4 neuron => 4 outputs (classes)\n",
    "    theta_1 = np.array([np.sin(np.arange(0, 5.9, 0.5))]).reshape((4, 3))\n",
    "    theta_2 = np.array([np.sin(np.arange(0, 5.9, 0.3))]).reshape((4, 5))\n",
    "    X = np.array([np.sin(np.arange(1,17))]).reshape([8, 2]);\n",
    "    # Accumulate predictions\n",
    "    preds = np.zeros((len(X), theta_2.shape[0]))\n",
    "    for i, sample in enumerate(X):\n",
    "        preds[i] = forward(sample, theta_1, theta_2)[-1]\n",
    "    print(preds)\n",
    "    # Select only the maxes\n",
    "    max_p = np.argmax(preds, axis=1)\n",
    "    exp_p = np.array([[4, 1, 1, 4, 4, 4, 4, 2]]).T\n",
    "    assert np.array_equal(max_p+1, exp_p)\n",
    "\n",
    "test_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(a_units, thetas, y, _lambda=0.01):\n",
    "    \"\"\"Perform backpropagation to determine layer gradients.\n",
    "    \n",
    "    :arg a_units: activated unit values, where each row represents a \n",
    "        layer.\n",
    "    :arg thetas: Weight matrices.\n",
    "    :arg y: Target values.\n",
    "    :kwarg float _lambda: Regularization hyperparameter.\n",
    "    :return: Gradients calculated per weight matrix. Same dimensions\n",
    "        as the corresponding weight matrix, bias value included.\n",
    "    \"\"\"\n",
    "    Deltas = list()\n",
    "    # Calculate error between output layer and target values\n",
    "    delta_output = a_units[-1, np.newaxis] - y\n",
    "    # For each hidden layer, calculate gradient _going backwards_\n",
    "    for i in range(len(a_units)-1, 0):\n",
    "        \n",
    "    \n",
    "    return Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, _lambda=0.01):\n",
    "        \"\"\"Initialize the neural network's hyperparameters.\"\"\"\n",
    "        self._lambda = _lambda\n",
    "        self.layers = layers\n",
    "        self.thetas = None\n",
    "        \n",
    "    def init_layers(self, X, layers):\n",
    "        \"\"\"Initialize theta values for the layers inside the network.\"\"\"\n",
    "        # Determine theta matrice values\n",
    "        # Randomly initialize weights\n",
    "        return np.zeros((1,1))\n",
    "\n",
    "    def train(self, X, y, verbose=False):\n",
    "        \"\"\"Train a neural network. \n",
    "\n",
    "        :arg matrix X: Training data points.\n",
    "        :arg vector y: Training data outcomes.\n",
    "        :arg [int] layers: Number of layers. The size of the first layer will\n",
    "            be initialized from the # of columns in X. The size of y will not \n",
    "            be used, in case an array of predictions is returned.\n",
    "        :kwarg float _lambda: Regularization parameter.\n",
    "        :kwarg bool verbose: Verbosity.\n",
    "        \"\"\"\n",
    "        if not self.thetas:\n",
    "            self.thetas = init_layers(X, self.layers)\n",
    "        for sample in X:\n",
    "            # Forward propagation\n",
    "            h = self.forward(sample)\n",
    "            # Backward's propagation\n",
    "            gradients = self.backward(h)\n",
    "            # Gradient checking using grad_approx\n",
    "            approx_grads = grad_approx(h) \n",
    "        # Gradient Descent at the end? Or every iteration?\n",
    "        result = fmin_cg()\n",
    "        k\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation for a single data point.\n",
    "        \n",
    "        :arg ndarray x: Data point as a row vector.\n",
    "        :return: Column vector of predictions. Length is dependent\n",
    "            on the number of rows in the last theta matrix.\n",
    "        \"\"\"\n",
    "        return np.zeros((1,1))\n",
    "    \n",
    "    def backward(self, preds):\n",
    "        \"\"\"Backwards propagate a set of predictions through the network.\n",
    "        \n",
    "        The error of the predictions, expressed by gradients, will be used to \n",
    "        tune the weights for each layer.\n",
    "        \n",
    "        There is no output, as the model weights are updated in-place.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on the dataset.\n",
    "        \n",
    "        Very similar to forward propagation, except the prediction probabilities\n",
    "        are translated into a vectorized binary outcome.\n",
    "        \"\"\"\n",
    "        preds = np.zeros(X.shape)\n",
    "        assert preds.shape == (X.shape[0], 1) # assert it's a column vector\n",
    "        maxes = np.argmax(preds, axis=1)\n",
    "        # Assert exactly one class was predicted \n",
    "        for p in maxes:\n",
    "            assert p.sum() == 1\n",
    "        return maxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
