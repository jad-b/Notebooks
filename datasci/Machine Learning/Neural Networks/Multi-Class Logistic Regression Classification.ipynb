{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.optimize import minimize, fmin_cg\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_dummy_ones(X):\n",
    "    return np.c_[np.ones(len(X)), X]\n",
    "\n",
    "def print_shape(name, X):\n",
    "    print(\"{name}: {X.shape}\".format(name=name, X=X))\n",
    "    \n",
    "def print_val(name, val):\n",
    "    print(\"{name}: {val}\".format(name=name, val=val))\n",
    "    \n",
    "def _obs_exp(a, b):\n",
    "    return \"\\nObserved: {}\\nExpected: {}\".format(a, b)\n",
    "\n",
    "def assert_shape(a, x):\n",
    "    assert a.shape == x, _obs_exp(a.shape, x)\n",
    "    \n",
    "def assertClose(a, b, tol):\n",
    "    assert np.allclose(a.flatten(), b.flatten(), atol=tol), \\\n",
    "        _obs_exp(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_predict_one_vs_all():\n",
    "    all_theta = np.array([\n",
    "            [1, -6, 3],\n",
    "            [-2, 4, -3]\n",
    "        ])\n",
    "    X = np.array([\n",
    "            [1, 7],\n",
    "            [4, 5],\n",
    "            [7, 8],\n",
    "            [1, 4]\n",
    "        ])\n",
    "    X = add_dummy_ones(X)\n",
    "#     print_val('X w/ bias', X)\n",
    "    ans = predict_one_vs_all(all_theta, X)\n",
    "    exp_ans = np.array([[1, 2, 2, 1]]).T  # Column vector\n",
    "    assert np.array_equal(ans, exp_ans), \\\n",
    "        \"\\nAnswer: {}\\nExpected: {}\".format(ans, exp_ans)\n",
    "\n",
    "        \n",
    "def predict_one_vs_all(all_theta, X):\n",
    "    \"\"\"Predict using a one vs all classifier.\"\"\"\n",
    "    m, n = X.shape  # # of features\n",
    "    num_labels = all_theta.shape[0] # # of classes\n",
    "    assert all_theta.shape[1] == n, \\\n",
    "        \"\\nTheta_all: {}\\nX: {}\".format(all_theta.shape, X.shape)\n",
    "      \n",
    "    # The sigmoid function was useful for collapsing real-valued numbers\n",
    "    # into a 0-1 probability for if something is or is not of class 'y'.\n",
    "    # Now we can just select from the \n",
    "    preds = X.dot(all_theta.T)\n",
    "    assert_shape(preds, (m, num_labels))\n",
    "#     print_val('Predictions', preds)\n",
    "#     print_val('Predictions, sigmoid\\'d', sigmoid(preds))\n",
    "    \n",
    "    maxes = np.argmax(preds, axis=1)  # Find maxes across rows\n",
    "    # Since we're using classifier index to represent the digit, we need \n",
    "    # to offset by one to align our predictions.\n",
    "    # Example:\n",
    "    #   theta_all[0] = classifier for digit 1\n",
    "    #   ...\n",
    "    #   theta_all[9] = classifier for digit 10\n",
    "    maxes += 1  # numpy makes this easy\n",
    "    maxes = maxes[np.newaxis].T  # Convert to column vector\n",
    "#     print_val('Predictions, max\\'d', maxes)\n",
    "    assert_shape(maxes, (m, 1))\n",
    "    return maxes\n",
    "\n",
    "\n",
    "test_predict_one_vs_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_one_vs_all():\n",
    "    X = np.array([\n",
    "        [8.00000,   1.00000,   6.00000],\n",
    "        [3.00000,   5.00000,   7.00000],\n",
    "        [4.00000,   9.00000,   2.00000],\n",
    "        [0.84147,   0.90930,   0.14112],\n",
    "        [0.54030,  -0.41615,  -0.98999]\n",
    "    ])\n",
    "    X = add_dummy_ones(X)\n",
    "    y = np.array([1, 2, 2, 1, 3])\n",
    "    num_labels = 3;\n",
    "    _lambda = 0.1;\n",
    "    all_theta = one_vs_all(X, y, num_labels, _lambda);\n",
    "    exp_all_theta = np.array([\n",
    "      [-0.559478,   0.619220,  -0.550361,  -0.093502],\n",
    "      [-5.472920,  -0.471565,   1.261046,   0.634767],\n",
    "      [ 0.068368,  -0.375582,  -1.652262,  -1.410138]\n",
    "    ])\n",
    "    assertClose(all_theta, exp_all_theta, tol=3.01e-3)\n",
    "\n",
    "\n",
    "def one_vs_all(X, y, num_labels, _lambda, iters=None):\n",
    "    \"\"\"One-vs-all multi-class classifier.\"\"\"\n",
    "    # Hold our collection of parameters for each classifier \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Train a classifier for each label\n",
    "    # Start with zero for theta for all classifiers\n",
    "    all_theta = np.zeros((num_labels, n))\n",
    "    for i in range(num_labels): # Digits range from 1-10\n",
    "        # Select for that label from our y values\n",
    "        target_y = (y == (i+1)).astype(int)\n",
    "        # Minimize using Conjugate Gradient\n",
    "        result = minimize(\n",
    "            fun=lr_cost_function,  # use our cost function to evaluate\n",
    "            x0=all_theta[i%num_labels],  # Initial guess for model parameters\n",
    "            args=(X, target_y, _lambda),  # Passed into fun\n",
    "            method='CG', # Conjugate Gradient\n",
    "            jac=lr_gradient,\n",
    "            options={'maxiter': iters}  # Same limit as Octave assignment\n",
    "        )  \n",
    "        all_theta[i%num_labels] = result.x\n",
    "        \n",
    "    return all_theta\n",
    "\n",
    "\n",
    "test_one_vs_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_lr_cost_function():\n",
    "    test_theta = np.array([-2, -1, 1, 2])\n",
    "    test_X = np.array([\n",
    "       [1, 8, 1, 6],\n",
    "       [1, 3, 5, 7],\n",
    "       [1, 4, 9, 2]\n",
    "    ])\n",
    "    test_y = np.array([1, 0, 1])\n",
    "    test_lambda = 3;\n",
    "    J = lr_cost_function(test_theta, test_X, test_y, test_lambda);\n",
    "    assertClose(np.array([J]), np.array([7.6832]), 1e-4)\n",
    "    \n",
    "def lr_cost_function(theta, X, y, _lambda=0.1):\n",
    "    m = len(y)\n",
    "    onez = np.ones(m)  # 1 x m (row vector)\n",
    "    \n",
    "    h = sigmoid(theta.dot(X.T))  # 1 x n * n x m => 1 x m; row vector of probabilities\n",
    "#     print_val('h', h)\n",
    "    # When y is negative, i.e. 0, this term goes away due to 'y*log(h)'\n",
    "    y_pos = np.negative(np.log(h).dot(y)) # 1 x m * m x 1 => 1 x 1 (scalar)\n",
    "#     print_val('y+', y_pos)\n",
    "    #     print_shape('y+', y_pos)\n",
    "    # When y is positive, i.e. 1, this term goes away due to 1-y\n",
    "#     print_val('1-h', onez-h)\n",
    "#     print_val('1-y', onez-y)\n",
    "    y_neg = (np.log(onez-h)).dot(onez-y) # 1 x m * m x 1 => 1 x 1 (scalar)\n",
    "#     print_val('y-', y_neg)\n",
    "    #     print_shape('y-', y_neg)\n",
    "    J = (1/m) * (y_pos - y_neg)\n",
    "#     print_shape('J', J)\n",
    "    J += (_lambda/(2*m)) * np.square(theta[1:]).sum()\n",
    "#     print_shape('lambda/2m * theta^2', reg)\n",
    "    return J\n",
    "        \n",
    "\n",
    "test_lr_cost_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_lr_gradient():\n",
    "    test_theta = np.array([-2, -1, 1, 2])\n",
    "    test_X = np.array([\n",
    "       [1, 8, 1, 6],\n",
    "       [1, 3, 5, 7],\n",
    "       [1, 4, 9, 2]\n",
    "    ])\n",
    "    test_y = np.array([1, 0, 1])\n",
    "    test_lambda = 3;\n",
    "    grad = lr_gradient(test_theta, test_X, test_y, _lambda=test_lambda)\n",
    "    exp_grad = np.array([0.31722, -0.12768, 2.64812, 4.23787])\n",
    "    assertClose(grad, exp_grad, 1e-4)\n",
    "\n",
    "    \n",
    "def lr_gradient(theta, X, y, _lambda=None, loud=False):\n",
    "    \"\"\"Logistic regression gradient.\"\"\"\n",
    "    m = len(y)\n",
    "    assert_shape(y, (m,)) # row vector of truths\n",
    "    \n",
    "    # Make predictions\n",
    "    h = sigmoid(theta.dot(X.T))  # 1 x n * n x m => 1 x m; row vector of probabilities\n",
    "    assert h.shape == (m,)\n",
    "    # Gradients (partial derivatives of cost function for theta)\n",
    "    #   Theta_j = 1/m * sum(h(x)-y)x\n",
    "    grad = (1/m) * ((h-y).dot(X)) # (1 x m - 1 x m) * m x n => 1 x n\n",
    "#     print_shape('gradient', grad)\n",
    "    grad[1:] += (_lambda/m) * theta[1:] # Skip theta_0\n",
    "    return grad\n",
    "\n",
    "\n",
    "test_lr_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logistic regression classifier\n",
      "% Accuracy:  96.44\n"
     ]
    }
   ],
   "source": [
    "# Now, actually try some predictions\n",
    "# Import the assignment data\n",
    "import scipy.io as sio\n",
    "ex3data = sio.loadmat('ex3/ex3data1.mat')\n",
    "X, y = ex3data['X'], ex3data['y'][:, 0]\n",
    "# Add a column of 1s for the bias weight (theta 0)\n",
    "X = add_dummy_ones(X)\n",
    "num_labels = len(np.unique(y))\n",
    "# Train classifiers\n",
    "all_theta = one_vs_all(X, y, num_labels, 0.1, iters=None)\n",
    "# Make our predictions\n",
    "predictions = predict_one_vs_all(all_theta, X)\n",
    "accuracy = np.mean(predictions[:, 0] == y) * 100\n",
    "print(\"Our logistic regression classifier\")\n",
    "print(\"% Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scikit-learn's Logistic Regression classifier\n",
      "% Accuracy: 96.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(C=10.0)\n",
    "lr_classifier.fit(X, y)\n",
    "p = lr_classifier.predict(X)\n",
    "def print_accuracy(a, b):\n",
    "    print(\"% Accuracy:\", np.mean(a==b) * 100.)\n",
    "print(\"Using scikit-learn's Logistic Regression classifier\")\n",
    "print_accuracy(p, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
