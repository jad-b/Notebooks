{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing\n",
    "\n",
    "Analysing data in real-time\n",
    "* Twitter trends\n",
    "* Google Analytics\n",
    "* Intrusion detection\n",
    "\n",
    "Map-reduce is a *batch-processing* system\n",
    "* High(er) latency\n",
    "* Have to wait for entire dataset to complete\n",
    "\n",
    "## Storm\n",
    "Apache Project\n",
    "JVM-based\n",
    "* Users: Twitter, Flipboard, Weather, WebMD\n",
    "\n",
    "Five major keywords\n",
    "* Tuples\n",
    "    * Ordered list of elements\n",
    "    * For example, on Twitter, a tuple may be (<tweeter, tweet>)\n",
    "    * Or if we're tracking clicks (<URL, clicker-IP, date, time>)\n",
    "* Streams\n",
    "    * Potentially infinite sequence of tuples\n",
    "    * \n",
    "* Spouts\n",
    "    * Stream generator\n",
    "    * Also known as a reader, crawler, watcher\n",
    "    * Could generate multiple streams\n",
    "* Bolts\n",
    "    * Stream processor\n",
    "    * Modifies stream input into new stream output\n",
    "    * Most of the code you write in a Storm app\n",
    "* Topology\n",
    "    * A directed graph of spouts and bolts (and output bolts)\n",
    "    * Makes up a Storm app\n",
    "    * Cycles allowed - avoiding infinite loops up to the user\n",
    "    \n",
    "### More on Bolts\n",
    "Bolts have many operations that can be applied to streams:\n",
    "``Filter``: Forward only tuples which satisfy a condition\n",
    "``Join``: When receiving two streams, output pairs which satisfy a condition (think ``zip`` in Python)\n",
    "``Apply``/``transform``: Modify each tuple with a function\n",
    "BUT, bolts need to work fast; might have TBs of data coming in.\n",
    "So, we can parallelize bolts by turning them into sub-tasks.\n",
    "* Split incoming streams into tasks via a \"Grouping Strategy\"\n",
    "    * Shuffle Grouping\n",
    "        * Distribute tuples evenly (round-robin)\n",
    "    * Fields Grouping\n",
    "        * Distribute tuples using a subset of their fields\n",
    "            * IP addresses below 192.xxx.xxx.xxx go *here*, above go *there*\n",
    "    * All Grouping\n",
    "        * All tasks receive all input tuples\n",
    "        * Good for Joins, where you combine with another stream\n",
    "\n",
    "### Storm Cluster\n",
    "* Master node\n",
    "    * Elected via a leader election protocol\n",
    "    * Runs a daemon known as a *Nimbus*\n",
    "        * Distributes code around cluster\n",
    "        * Failure detection\n",
    "        * Assigns tasks to machines\n",
    "* Worker\n",
    "    * Runs on a machine\n",
    "    * Runs a *supervisor* daemon\n",
    "    * Listens for work assigned to its machines\n",
    "* Zookeeper\n",
    "    * Coordinates Nimbus and supervisors\n",
    "    * Backs up states of servers\n",
    "    \n",
    "#### Node Failures\n",
    "* A tuple has *failed* when its topology of resulting tuples fails to be fully processed within a timeframe\n",
    "    * Implies a sub-tree of the overall topology, originating from a root tuple\n",
    "* __Anchoring__ Map an output tuple to one+ input tuples. If the output tuple is not received, replay the input tuples.\n",
    "    * This code lives in ``OutputCollector``\n",
    "    * ``Emit``: Emit an output tuple\n",
    "    * ``Ack``: Acknowledge you finished processing a tuple\n",
    "    * ``Fail``: Immediately fail the spot tuple at root of tuple topology. Might do this if there was a database exception, etc.\n",
    "        * Must ``ack``/``fail`` every tuple. Otherwise you might end up with memory leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Graph Processing\n",
    "\n",
    "__Sample Systems__\n",
    "* Google's Pregel system\n",
    "* Piccolo, Giraph, GraphLab, PowerGraph, LFGraph, X-Stream\n",
    "\n",
    "#### What is a Graph?\n",
    "* A \"network\"\n",
    "* Take Facebook\n",
    "    * Vertices (or nodes) would be users\n",
    "    * Edges would be friend relationships\n",
    "* Take the WWW:\n",
    "    * Vertices: Routers/switches\n",
    "    * Edges: URL Links\n",
    "* \"Directed\" graphs are uni-directional\n",
    "* \"Bi-directional\" graphs are undirected\n",
    "    * Social networks tend to be this\n",
    "    \n",
    "#### Why do we need to process Graphs?\n",
    "* Need to analyse graphs to derive properties\n",
    "    * Shortest paths\n",
    "    * Matching\n",
    "    \n",
    "### Prototypical Graph Processing Algorithm\n",
    "\n",
    "This is for a non-distributed system.\n",
    "\n",
    "* Works in *iterations*\n",
    "* Assign each vertex a *value*\n",
    "* For each iteration, each vertex\n",
    "    * Gather values from immediate neighbors (one hope via an edge)\n",
    "    * Perform computation\n",
    "    * Update its value and broadcast new value to neighbors\n",
    "* Terminate after\n",
    "    1. Fixed number of iterations\n",
    "    1. Vertice values stop changing\n",
    "    \n",
    "#### Distributed\n",
    "\n",
    "We could use Hadoop (Map-Reduce)\n",
    "Each stage would be 1 graph iteration; as many map-reduce runs as their are iterations\n",
    "Assign vertex IDs as keys in the reduce phase\n",
    "Actually very slow; must transfer vertices over network, and write all these values to HDFS\n",
    "\n",
    "### Bulk Synchronous Parallel Model\n",
    "Each vertex is computed by a separate processor\n",
    "At end of computation on each vertex, wait for other vertices to finish, then proceed as a whole to the next stage.\n",
    "\n",
    "1. Assign each vertex to one server; thus, each server has many vertices\n",
    "1. For each iteration,\n",
    "    1. Gather\n",
    "        * Get all neighboring vertices' values\n",
    "    1. Apply\n",
    "        * Compute new value\n",
    "    1. Scatter\n",
    "        * Re-distribute new value\n",
    "        \n",
    "Now, the locality of neighboring vertices plays a role, as we have to communicate over the network during the Gather and Scatter phases.\n",
    "* Hash-based assignment\n",
    "    * hash(Vertex ID) % # of servers \n",
    "        * Much like Chord\n",
    "* Locality-based\n",
    "    * Assign vertices with more neighbors to the same server as its neighbors\n",
    "        \n",
    "### Pregel\n",
    "* Pregel uses the master/worker model\n",
    "    * Master\n",
    "        * Monitors worker servers\n",
    "        * Maintains membership list of worker servers\n",
    "        * Has Web UI\n",
    "    * Worker\n",
    "        * Runs Gather-Apply-Scatter\n",
    "* Uses Google File System or BigTable\n",
    "* Temp data stored on disk\n",
    "\n",
    "#### Execution\n",
    "1. Many copies of program begin executing on cluster\n",
    "1. Master assigns a partition of vertices to each worker\n",
    "1. Master tells all workers to perform one iteration\n",
    "1. Master waits for all workers to finish before initiating next iteration\n",
    "1. Computation halts once all vertices are inactive or no messages are in transit\n",
    "1. Master instructs each worker to save its portion of the graph.\n",
    "\n",
    "#### Fault-Tolerance\n",
    "1. Checkpointing\n",
    "    * Workers periodically snaphost their partitions to persistent storage\n",
    "1. Failure Detection\n",
    "    * Ping messages from master->worker\n",
    "1. Recovery\n",
    "    * Master reassigns graph partitions to currently available workers\n",
    "    * Workers all reload their partition state from last checkpoint\n",
    "    \n",
    "#### Performance\n",
    "* Single-Source Shortest Path\n",
    "    * 1 B vertices\n",
    "        * 50 workers = 180 seconds\n",
    "        * 800 workers = 20 seconds\n",
    "    * 50 B vertices\n",
    "        * 800 workers = 700 seconds (< 12 minutes!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Structure\n",
    "\n",
    "Network == Graph (synonyms)\n",
    "\n",
    "Graphs have __nodes__, or __vertices__\n",
    "__Edges__ connects nodes.\n",
    "\n",
    "\n",
    "### Complexity\n",
    "1. Structural\n",
    "    * Size and relations between nodes\n",
    "1. Evolution\n",
    "    * Churn and change in networks\n",
    "1. Diversity\n",
    "    * Variance in edges per node\n",
    "    * Variance in node *weight* and edge *cost*\n",
    "    * Some people are more popular, some friendships are more important\n",
    "1. Node Complexity\n",
    "    * Attribute and schema differences of nodes\n",
    "1. Emergent Phenomena\n",
    "    * Simple end behavior becomes complex system behavior\n",
    "    * Butterfly effect\n",
    "    \n",
    "__small world networks__Any node can be reach by any other with a small number of hops\n",
    "* Why? Networks _evolve naturally_ from a starting nucleus\n",
    "\n",
    "### Characterizing Networks\n",
    "1. Clustering Coefficient (CC)\n",
    "    * Given three vertices, A, B, C, an A-C edge, and a C-B edge, what is the probability there is an edge A-B?\n",
    "    * Tree networks have a CC of 0 - no three nodes are all directly connected\n",
    "    * Complete graph (every vertex connected to every other vertex) has a CC of 1.0\n",
    "    * Random Graph: Low CC, Short Paths\n",
    "        * ![](img/random_graph.png)\n",
    "    * Extended Ring Graph: High CC, Long Paths\n",
    "        * ![](img/extended_ring_graph.png)\n",
    "    * Small World Network: High CC, Short path\n",
    "1. Path Length\n",
    "    * Between any pair of vertices in the graph, what is the shortest path between them?\n",
    "    * Calculate path lengths for every pair of nodes and take the average = average path length\n",
    "\n",
    "You can convert an extended ring or a random graph into a small-world network\n",
    "![](img/small_world_chart.png)\n",
    "\n",
    "Most \"naturally evolved\" networks are small-world.\n",
    "Thus, they also grow incrementally\n",
    "* Preferntial model of growth\n",
    "    * When adding a vertex `u` to a graph `G`, connect it to existing vertex `v` with a probability proportional to `|v.neighbors|`\n",
    "    \n",
    "#### Degrees\n",
    "__Degree of a vertex__ # of immediate neighbor vertices\n",
    "__Degree Distribution__ Probability of a given node having `k` edges\n",
    "\n",
    "* Regular graphs: all node have the same degree\n",
    "* Many distributions can emerge: Gaussian, Random, Power law\n",
    "![Power Law Graphs](img/power_law_graphs.png)\n",
    "\n",
    "As you can see, graphs following a power law have a few nodes with many neighbors.\n",
    "\n",
    "Examples of power law & small-world graphs:\n",
    "\n",
    "* Telephone call graph, protein networks??\n",
    "* WWW is small-world & power-law with $\\alpha = 2.1 - 2.4$\n",
    "\n",
    "_But_, power-law != small-world\n",
    "\n",
    "#### Resilience of Small-World Graphs\n",
    "* Kill a large random selection of nodes = graph is likely to stay connected\n",
    "* Kill a few high-degree nodes = likely to disconnect graph\n",
    "    * Body relies on a few key nutrients as building blocks for many other nutrients\n",
    "    * Certain key cities route large amounts of electric in the grid\n",
    "\n",
    "* But you don't just have to kill a high-degree node; you could also overload them with shortest-path seletions.\n",
    "    * Solution(possibly): Sometimes take a random path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Ideas\n",
    "\n",
    "* Parallelism\n",
    "    * Storm Bolts run many tasks in parallel\n",
    "    * Pregel uses many workers, coordinated by Pregel\n",
    "* Topologies (Directed Graphs)\n",
    "* Fault-tolerance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
