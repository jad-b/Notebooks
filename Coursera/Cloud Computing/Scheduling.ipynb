{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling\n",
    "\n",
    "## Single-processor scheduling\n",
    "Problem: We have a lot of tasks to run\n",
    "System Model: \n",
    "    * Limited # of processors\n",
    "    * Finite amount of memory, disk, network bandwidth\n",
    "Objectives:\n",
    "    * Good throughput of jobs per time unit\n",
    "        * Related: job completion time\n",
    "    * High resource utilization\n",
    "        * Don't leave any resources idle\n",
    "        \n",
    "### Strategies\n",
    "* FIFO\n",
    "    * Run tasks sequentially\n",
    "    * Completion time: $\\frac{\\Sigma (tasks)}{|tasks|}$\n",
    "* Shortest Task First\n",
    "    * Maintain priority queue of tasks\n",
    "        * Priority: Ascending job run time\n",
    "    * Formally provable to give smallest job completion time\n",
    "        * Could also use a custom priority\n",
    "* Round-Robin\n",
    "    * Evenly divide tasks into _quanta_ and schedule these separately\n",
    "    * Run portion of tasks on queue\n",
    "    * Preferable for interactive apps requiring quick responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Tenant System\n",
    "\n",
    "__tenant__ Users or jobs\n",
    "__container__ Set of resources sufficient to run one task of one job. Basic cluster unit.\n",
    "\n",
    "#### Desires for a Multi-Tenant schedule\n",
    "* We need to use a system to allocate resources (nodes, containers, processor time) to tasks (and their sub-tasks)\n",
    "* We want to be fair(ish) to all tenants; can emphasize fairness if that's an important principle\n",
    "* Want to use as much of our system as we can, all the time.\n",
    "\n",
    "### Hadoop Capacity Scheduler\n",
    "* Multiple queue system\n",
    "* Each queue is guaranteed some portion of the cluster capacity\n",
    "    * Q1 gets 80% of the cluster\n",
    "    * Q2 gets 20% of the cluster\n",
    "    * High priority jobs go to Q1\n",
    "* Queues are now in charge of running jobs on given resources\n",
    "\n",
    "#### Attributes\n",
    "* Queues are typically FIFO\n",
    "* Queues can be hierarchical\n",
    "* Admins can configure queues\n",
    "    * Add limits\n",
    "        * Soft limit: Min % of cluster resources queue is guaranteed\n",
    "        * Hard limit: Max % of cluster resources\n",
    "    * Elasticity: Ability to occupy add'l resources, if cluster load is low\n",
    "* Users can specify requirements\n",
    "    * Must have $x$ memory available, etc.\n",
    "* Pre-emption is not allowed!\n",
    "    * If load is high, you must wait out task completion, then remove resource access from that queue.\n",
    "\n",
    "\n",
    "### Hadoop Fair Scheduler\n",
    "* Goal: All jobs get an equal share of resources\n",
    "* One job gets entire cluster\n",
    "* As jobs arrive, the cluster is evenly divided between jobs\n",
    "\n",
    "#### Attributes\n",
    "* Divides cluster into pools\n",
    "    * Usually, one user per pool\n",
    "* Resources are evenly divided between pools.\n",
    "* Pool scheduling can be FIFO, FCFS, fair-sharing, whatever (configurable)\n",
    "* Pools can have _min shares_\n",
    "    * Guaranteed min % of cluster \n",
    "* When a min share is not met\n",
    "    * If min share is not provided within a _timeout_\n",
    "        * Pre-empt running tasks in other pools\n",
    "            * a.k.a, kill 'em\n",
    "            * OK - tasks are idempotent. Just a little wasteful.\n",
    "        * Resource Scheduler kills most-recently-started tasks\n",
    "            * Minimizes waste\n",
    "        * Community has proposed using a saved-state pre-emption\n",
    "            * Save the pre-empted tasks state for resuming later\n",
    "            * Common in grid-computing\n",
    "\n",
    "### Estimating Task Lengths\n",
    "* We'd rather use Shortest-Task-First (STF) for scheduling, as it's optimal\n",
    "* But that means we need to estimate task length\n",
    "\n",
    "#### Estimation Approaches\n",
    "* By job: $Run Time \\propto Input Size$\n",
    "* Across jobs: $Run Time \\propto$ Average of run times of sibling tasks within parent job\n",
    "    * Can weight averaged tasks by input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominant Resource-Fair Scheduling\n",
    "\n",
    "Scheduling VMs in a cloud (cluster)?\n",
    "* Jobs may have multi-resource requirements\n",
    "    * Job 1 may need 2 CPU's, 8GB RAM\n",
    "    * Job 2 may need 6 CPU's, 2GB RAM\n",
    "* What _is_ fairness?\n",
    "\n",
    "### Dominant Resource Fairness (DRF)\n",
    "* Out of UC Berkeley\n",
    "* Proven to be fair for multi-tenant systems\n",
    "* Can't be gamed by individual tenants; requeusting more resources only penalizes\n",
    "* Envy-free: No tenant can eyeball another tenant's allocation\n",
    "* Works for scheduling VMs & Hadoop\n",
    "* Used in Mesos\n",
    "    * A cloud environment OS\n",
    "    \n",
    "### System Model\n",
    "* 18 CPUs\n",
    "* 36 GB RAM\n",
    "\n",
    "#### Example\n",
    "* Using Job examples from above,\n",
    "    * Job 1 need $2/18 CPUs = 1/9$\n",
    "    * Job 1 needs $8/36 RAM = 2/9$\n",
    "        * _RAM_ intensive\n",
    "    * Job 2 needs $6/18 CPUs = 1/3$\n",
    "    * Job 2 needs $2/36 RAM = 1/18$\n",
    "        * _CPU_ intensive\n",
    "* DRF guarantees the % of its dominant resource type that it gets cluster-wide is the same for all jobs\n",
    "    * Job 1's % of RAM = Job 2's % of CPU\n",
    "* Solution: \n",
    "    * Job 1 gets 3 tasks with <2 CPUs, 8 GB> each\n",
    "        * Job 1's % of RAM: $3 * 8/36 = 2/3$\n",
    "    * Job 2 gets 2 tasks with <6 CPUs, 2 GB> each\n",
    "        * Job 2's % of CPU: $2 * 6/18 = 2/3$\n",
    "* DRF generalizes to multiple jobs\n",
    "* DRF also generalizes to more than 2 resource types\n",
    "    * CPU, RAM, Network, Disk, etc.\n",
    "* _Guarantees_ each job gets a fair share of its dominant resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* STF works the fastest - but you have to be able to estimate task length\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
