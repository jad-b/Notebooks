{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassandra\n",
    "\n",
    "\n",
    "### Glossary\n",
    "\n",
    "Replica: Node storing _a_ value for a key\n",
    "\n",
    "__Replication:__\n",
    "1. Simple\n",
    "  1. Random: Just like Chord, use a hash to figure out where each key goes\n",
    "  2. ByteOrdered: Assign a range of keys to each server\n",
    "      1. Good for when you do a lot of ranged lookups (give me all users a-k)\n",
    "1. NetworkTopology (Multi-Datacenter deployments)\n",
    "  1. Two/Three replicas per DC\n",
    "  2. Per DC:\n",
    "      1. First replica gets dropped by Partitioner\n",
    "      1. Search around the ring until you get a server on a different rack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Snitchin'__\n",
    "* Informs on network topology\n",
    "* A few options:\n",
    "    * Simple: Rack-unaware\n",
    "    * RackInferring: Assumes IP addresses have rack info:\n",
    "        * `x.<DC>.<rack>.<node>`\n",
    "    * PropertyFile: Use a config file, holy shit\n",
    "    * EC2Snitch\n",
    "        * EC2 Region = DC\n",
    "        * Availability zone = rack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Readin' & Writin'__\n",
    "\n",
    "* Can't lock on writes\n",
    "* Write path:\n",
    "    * Client tries to write to a Cassandra Coordinator (some node in the ring)\n",
    "    * Coordinator uses the Partitioner to output query to all need-to-know nodes in teh ring\n",
    "    * Coordinator responds to client once _X_ nodes respond\n",
    "    \n",
    "__Fault tolerance__\n",
    "* Coordinator buffers writes if any replica is down (doesn't respond)\n",
    "* ^This is called \"hinted handoff\"\n",
    "* If you've got a multi-DC setup, you might:\n",
    "    * Elect a per-DC Coordinator (distinct from a Coordinator who talks to clients)\n",
    "    * Who is the DC Coordinator? They get elected by Zookeeper\n",
    "        * Zookeeper (I knew you'd ask) is running a consensus-algo called Paxos\n",
    "\n",
    "__So you're a replica node__\n",
    "* And you just got a write\n",
    "* You will\n",
    "    1. Log the write in a disk commit log\n",
    "    1. Update a \"memtable\"\n",
    "        1. memtable = in-memory repr of multiple k-v pairs\n",
    "        1. Key-indexed cache\n",
    "        1. Is Write-back, not write-through\n",
    "            1. Write-back: Stored in memory\n",
    "            1. Write-through: Store in mem & disk\n",
    "        1. Gets flushed to disk periodically\n",
    "    1. But you also need to store the file on disk anyway, so:\n",
    "        1. You'll have a data file: List of key-value pairs, sorted by key\n",
    "            1. A \"__String Sorted Table (SSTable)__\"\n",
    "        1. An _index file_: List of (key, position in data file)\n",
    "        1. And a Bloom Filter in front of that.\n",
    "* A what? Bloom Filter?\n",
    "    * Compact way repr'ing a set of items\n",
    "    * Existence checks become very cheap\n",
    "        * Has a risk of false positives\n",
    "        * But never false negatives\n",
    "    * Bascially a bitmap\n",
    "        * Initially all values are zero (no keys present)\n",
    "        * For each key, apply a set of hash algos\n",
    "            * Set all output bits in the bitmap to 1\n",
    "        * As you add keys, you start to have over-lapping 1 bits\n",
    "        * This is where your risk of false positives comes from\n",
    "        * But it's still pretty low:\n",
    "            * If you use k=4 hash functions\n",
    "            * A 3200 bitmap\n",
    "            * And insert a 100 items,\n",
    "            * You only have a 0.02% False Positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compaction__\n",
    "* Each server ends up having many SSTables\n",
    "* A single key may end up in several SSTables\n",
    "* This is wasteful\n",
    "* Periodically, merge key entries across SSTables into one.\n",
    "    * Use most recent key entry\n",
    "    \n",
    "\n",
    "__Deletes__\n",
    "* Don't delete anything immediately\n",
    "* Instead, mark for deletion with a _tombstone_\n",
    "* Compaction deletes tombstoned entries\n",
    "\n",
    "__Readin'__\n",
    "* Coordinator asks X replicas\n",
    "* Favors quick-responding replicas from past data\n",
    "* Then returns the latest-timestamped value (Replicas may not be consistent)\n",
    "\n",
    "__Consistency__\n",
    "* Coordinator does background checks for older values\n",
    "    * Checks replicas for keys\n",
    "    * Updates older replicas with most recent key information\n",
    "* What if compaction takes too long? \n",
    "    * You can get columns for a key separated across multiple SSTalbles\n",
    "        * Say you just update one column of the key\n",
    "    * Reads start to hit multiple SSTables\n",
    "    * Slow-er, but still fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membership\n",
    "\n",
    "Any server in the cluster could act as a Coordinator.\n",
    "\n",
    "Every server needs to know about every other server\n",
    "\n",
    "Cassandra uses Gossip-style membership.\n",
    "1. Nodes periodically gossip their membership list\n",
    "1. On receipt, local membership lists get updated\n",
    "    1. If any node in the membership list has a heartbeat that's older that `time_to_fail`, then mark the node as failed.\n",
    "    \n",
    "### Suspicion Mechanism\n",
    "* Here \"suspicion\" means figuring out if a node has failed\n",
    "* Having a mechanism lets us set these suspect/fail timeouts adaptively.\n",
    "* Cassandra uses an \"Accrual Detector\"\n",
    "    * The Accrual Detector outpus a value PHI\n",
    "    * PHI represents the probability a node failed\n",
    "    * PHI takes into account how long messages have historically taken to be received from the suspected node\n",
    "        * PHI(t) = -log(P(t_now - t_last) / log 10\n",
    "    * Apps set a PHI threshold instead of hard & fast timeouts. \n",
    "\n",
    "## X: The Cap Theorem\n",
    "\n",
    "Basically, you can have 2 out of these 3:\n",
    "1. __C__onsistency. All nodes see the same data OR all reads return the latest data\n",
    "1. __A__vailability. All operations are always avilable, and quickly\n",
    "1. __P__artition Tolerance. Ability of system to continue normal functioning if you remove a piece of the system.\n",
    "\n",
    "Since Partition Tolerance is an absolute requirement, you see __Cassandra__ choosing availability, with traiditonal __RDBMS__ valuing consistency but offering poor parition tolerance.\n",
    "\n",
    "### Eventual Consistency\n",
    "* If you stop writing to a key, all of its value (replicas) will eventually be the same\n",
    "* In actual systems with continual writes, you have a lagging \"wave\" of updated values being pushed to all servers\n",
    "* If you can have low-write periods, then the system will converge quickly to a consistent state.\n",
    "\n",
    "__Consistency Levels__\n",
    "* Cassandra lets you choose how consistent each operation can be\n",
    "* ALL: First server that writes and returns is accepted. If replicas are down, just cache the write on the Coordinator and signal success back to the client. \n",
    "* ANY: Wait for each replica to ACK receipt of write. \n",
    "    * End up waiting for slowest replica\n",
    "    \n",
    "Next, you have a spectrum of _how many_ replicas must ACK\n",
    "\n",
    "* ONE: Wait for at least one replica to ACK write receipt\n",
    "* QUORUM: At least 50% of replicas ACK write receipt\n",
    "    * Faster than all\n",
    "    * Pretty strong consistent\n",
    "\n",
    "#### QUORUM \n",
    "\n",
    "__Reads__\n",
    "* Client specifies R replicas to receive values from\n",
    "* Coordinator waits for R replicas to respond before itself responding\n",
    "* Coordinator initiates background read-repair for remaining N-R replicas.\n",
    "\n",
    "__Writes__\n",
    "* Client specifies W, number of replicas to receive value\n",
    "* Coordinator can:\n",
    "    1. Block until W replicas reply\n",
    "    1. ACK immediately back to the client, and follow-up with making sure W replicas have been written to.\n",
    "    \n",
    "__Ensuring Strong Consistency__\n",
    "1. W + R > N\n",
    "    * Make sure at least one server is shared by both read & write quorums\n",
    "1. W > N/2\n",
    "    * Greater than 50% of replicas are used for each write\n",
    "        * Detects conflicts\n",
    "        * Ensures replication\n",
    "\n",
    "Cassandra lets you choose several level of QUORUM:\n",
    "1. QUORUM: Quorum across all replicas in all DCs\n",
    "1. LOCAL_QUORUM: Quorum in coordinator's DC\n",
    "1. EACH_QUORUM: Let each DC find its own QUORUM, reply up a hierarchy of potential QUORUMs before reaching Coordinator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Consistency Spectrum\n",
    "\n",
    "Consistency is on a spectrum:\n",
    "```\n",
    "Faster R/W <---------------------------------------- Slower R/W\n",
    "Eventual <===========================================> Strong\n",
    "```\n",
    "There are many models that have been derived upon this spectrum\n",
    "\n",
    "### Eventual Consistency Models\n",
    "\n",
    "__Per-key Sequential__ \n",
    "* On a per-key basis, all ops have a global order\n",
    "* maintain a per-key Coord., serialize all ops through that Coord.\n",
    "\n",
    "__CRDT__\n",
    "* Commutative Replicated Data Type\n",
    "* Storing data in a way where you can reverse the order of two ops and get the same result\n",
    "* Imagine your value is an int, and your op(s) are just +1\n",
    "\n",
    "__Red-Blue Consistency__\n",
    "* _Blue_ ops can be executed in any order across DCs\n",
    "    * Commutative\n",
    "* _Red_ ops must be executed in the same order.\n",
    "    * Serial\n",
    "\n",
    "__Causal__\n",
    "* Newer notion out of CMU\n",
    "* \"Reads must respet partial order based on info flow\"\n",
    "* Reads must respect causality\n",
    "* Reads must return most recently-written value\n",
    "\n",
    "### Strong Consistency Models\n",
    "\n",
    "__Linearizability__\n",
    "* Each op by a client is visible/available instantaneously to all other clients\n",
    "\n",
    "__Sequential__\n",
    "* Any execution results is the same as if the ops were executed in a sequential order\n",
    "* Ops can be re-ordered after-the-fact to facilitate a consistent view across all clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase\n",
    "\n",
    "* Google's BigTable system was \"blob-based\"\n",
    "* Yahoo! open-sourced it to HBase\n",
    "* __Features__:\n",
    "    * Get/Put (row)\n",
    "    * Scan(row range, filter)\n",
    "    * MultiPut\n",
    "* Prefers consistency over availability\n",
    "\n",
    "![HBase Architecture](img/HBase_architecture.png)\n",
    "\n",
    "### HBase Components\n",
    "\n",
    "* Client\n",
    "    * Sends R/W\n",
    "* Zookeeper\n",
    "    * Coordinates client & server communication\n",
    "    \n",
    "* HBaseTable\n",
    "    * Table of information\n",
    "    * Gets split into different regions for _replication_\n",
    "* ColumnFamily\n",
    "    * Subset of colunns inside HBaseTable\n",
    "    * All regions of an HBaseTable contain the same set of columns between their ColumnFamilies\n",
    "* HRegionServer\n",
    "    * Contains HRegions: Paritions of an HBase Table\n",
    "        * Store\n",
    "            * One Store per combination of ColumnFamily + region \n",
    "            * MemTable\n",
    "                * In-memory key-values\n",
    "                * Flushed to disk once full\n",
    "            * StoreFiles\n",
    "                * Where data lives on disk\n",
    "            * HFile\n",
    "                * File stored in Hadoop Distributed File System\n",
    "                \n",
    "![HFile Structure](img/HFile.png)\n",
    "\n",
    "### Write-Ahead Log\n",
    "\n",
    "![HBase Write-Ahead Log](img/hbase_write_ahead_log.png)\n",
    "\n",
    "1. Client writes four keys to HRegionServer\n",
    "1. HRegionServer directs (k1, k2) to HRegion1, (k3, k4) to HRegion2\n",
    "1. HRegion# servers write to HLog _before_ writing to Store::MemStore\n",
    "\n",
    "__On failure__\n",
    "1. HMaster/HRegionServer: Replay HLog\n",
    "    1. Add edits to appropriate HRegion MemStores\n",
    "        * These will get flushed to HFiles once full\n",
    "        \n",
    "### Master/Slave\n",
    "\n",
    "* Single Master cluster\n",
    "* Slave clusters replicate Master's tables\n",
    "* Master sends HLogs to slave clusters\n",
    "* Zookeeper used to store/control stateful information\n",
    "    * Zookeepers presents URLs for storing/retrieving info\n",
    "        * Example: /hbase/replication/rs/<log id>\n",
    "    * Service discovery / key-value store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
